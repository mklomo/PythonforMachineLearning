{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting\n",
    "\n",
    "The algorithm that we are going to implement in this section will allow us to combine different classification\n",
    "algorithms associated with individual weights for confidence. Our goal is to build a stronger\n",
    "meta-classifier that balances out the individual classifiers’ weaknesses on a particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10-fold cross validation\n",
      "\n",
      "roc_auc: 0.92 (+/- 0.15) [Logistic Regression]\n",
      "roc_auc: 0.87 (+/- 0.18) [Decision Tree]\n",
      "roc_auc: 0.85 (+/- 0.13) [KNN]\n",
      "roc_auc: 0.97 (+/- 0.10) [Ensemble Majority Voting]\n"
     ]
    }
   ],
   "source": [
    "# Import the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "\n",
    "X, y = iris.data[50:, [1,2]], iris.target[50:]\n",
    "\n",
    "\n",
    "# Initial the label Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform the labels\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "# Split the data into test and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# Create the classifiers\n",
    "clf_1 = LogisticRegression(penalty='l2', C=0.001, \n",
    "                           solver='lbfgs', \n",
    "                           random_state=1)\n",
    "\n",
    "\n",
    "clf_2 = DecisionTreeClassifier(max_depth=1,\n",
    "                               criterion='entropy',\n",
    "                               random_state=0\n",
    "                               )\n",
    "\n",
    "\n",
    "clf_3 = KNeighborsClassifier(n_neighbors=1,\n",
    "                             p=2,\n",
    "                             metric='minkowski'\n",
    "                             )\n",
    "\n",
    "\n",
    "# Ensemble Classifier\n",
    "eclf = EnsembleVoteClassifier(clfs=[clf_1, clf_2, clf_3], weights=[1,1,1], voting='soft')\n",
    "\n",
    "\n",
    "\n",
    "pipe_1 = Pipeline([['sc', StandardScaler()],\n",
    "                  ['clf_1', clf_1]]\n",
    "                  )\n",
    "\n",
    "pipe_2 = Pipeline([['sc', StandardScaler()],\n",
    "                   ['clf_2', clf_2]\n",
    "                   ])\n",
    "\n",
    "\n",
    "pipe_3 = Pipeline([['sc', StandardScaler()],\n",
    "                   ['clf_3', clf_3]\n",
    "                   ])\n",
    "\n",
    "\n",
    "pipe_4 = Pipeline([['sc', StandardScaler()],\n",
    "                   ['eclf', eclf]\n",
    "                   ])\n",
    "\n",
    "\n",
    "clf_labels = ['Logistic Regression', 'Decision Tree', 'KNN', 'Ensemble Majority Voting']\n",
    "\n",
    "print('\\n10-fold cross validation\\n')\n",
    "\n",
    "\n",
    "for clf, label in zip([pipe_1, pipe_2, pipe_3, eclf], clf_labels):\n",
    "    scores = cross_val_score(estimator = clf,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=10,\n",
    "                             scoring='roc_auc'\n",
    "                             )\n",
    "    \n",
    "    print(f'roc_auc: {scores.mean():.2f} '\n",
    "          f'(+/- {scores.std():.2f}) [{label}]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating and tuning the ensemble classifier\n",
    "\n",
    "> Remember that the test data set is not to be used for model selection; its purpose is merely to report an unbiased estimate of the generalization performance of a classifier system:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression': LogisticRegression(C=0.001, random_state=1),\n",
       " 'decisiontreeclassifier': DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0),\n",
       " 'kneighborsclassifier': KNeighborsClassifier(n_neighbors=1),\n",
       " 'logisticregression__C': 0.001,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__l1_ratio': None,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'auto',\n",
       " 'logisticregression__n_jobs': None,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'logisticregression__random_state': 1,\n",
       " 'logisticregression__solver': 'lbfgs',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False,\n",
       " 'decisiontreeclassifier__ccp_alpha': 0.0,\n",
       " 'decisiontreeclassifier__class_weight': None,\n",
       " 'decisiontreeclassifier__criterion': 'entropy',\n",
       " 'decisiontreeclassifier__max_depth': 1,\n",
       " 'decisiontreeclassifier__max_features': None,\n",
       " 'decisiontreeclassifier__max_leaf_nodes': None,\n",
       " 'decisiontreeclassifier__min_impurity_decrease': 0.0,\n",
       " 'decisiontreeclassifier__min_samples_leaf': 1,\n",
       " 'decisiontreeclassifier__min_samples_split': 2,\n",
       " 'decisiontreeclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'decisiontreeclassifier__random_state': 0,\n",
       " 'decisiontreeclassifier__splitter': 'best',\n",
       " 'kneighborsclassifier__algorithm': 'auto',\n",
       " 'kneighborsclassifier__leaf_size': 30,\n",
       " 'kneighborsclassifier__metric': 'minkowski',\n",
       " 'kneighborsclassifier__metric_params': None,\n",
       " 'kneighborsclassifier__n_jobs': None,\n",
       " 'kneighborsclassifier__n_neighbors': 1,\n",
       " 'kneighborsclassifier__p': 2,\n",
       " 'kneighborsclassifier__weights': 'uniform',\n",
       " 'clfs': [LogisticRegression(C=0.001, random_state=1),\n",
       "  DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0),\n",
       "  KNeighborsClassifier(n_neighbors=1)],\n",
       " 'fit_base_estimators': True,\n",
       " 'use_clones': True,\n",
       " 'verbose': 0,\n",
       " 'voting': 'soft',\n",
       " 'weights': [1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best params for the ensemble method\n",
    "\n",
    "eclf.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Based on the values returned by the get_params method, we now know how to access the individual\n",
    "classifier’s attributes. Let’s now tune the inverse regularization parameter, C, of the logistic regression\n",
    "classifier and the decision tree depth via a grid search for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decisiontreeclassifier__max_depth': 1, 'logisticregression__C': 0.0001}\n",
      "ROC AUC for training data:  0.97\n",
      "ROC AUC for test data:  0.97\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "            'decisiontreeclassifier__max_depth': [1,2],\n",
    "            'logisticregression__C': [0.0001, 0.001, 0.1, 100.0]\n",
    "}\n",
    "\n",
    "# Fit params to the Grid Search CV\n",
    "grid = GridSearchCV(estimator=eclf,\n",
    "                    param_grid=params,\n",
    "                    cv=10,\n",
    "                    scoring='roc_auc'\n",
    "                    )\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best params\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Print the Training Score\n",
    "print(f'ROC AUC for training data:  {grid.best_score_:.2f}')\n",
    "\n",
    "\n",
    "# Generalized Score on Test Data\n",
    "print(f'ROC AUC for test data:  {grid.score(X_test, y_test):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, we get the best cross-validation results when we choose a lower regularization strength\n",
    "(C=0.0001), whereas the tree depth does not seem to affect the performance at all, suggesting that a\n",
    "decision stump is sufficient to separate the data. To remind ourselves that it is a bad practice to use\n",
    "the test dataset more than once for model evaluation, we are not going to estimate the generalization\n",
    "performance of the tuned hyperparameters in this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging – building an ensemble of classifiers from bootstrap samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "\n",
    "df_wine = pd.read_csv(df_url, header=None)\n",
    "\n",
    "df_wine.columns = ['Class label', 'Alcohol',\n",
    "                    'Malic acid', 'Ash',\n",
    "                    'Alcalinity of ash',\n",
    "                    'Magnesium', 'Total phenols',\n",
    "                    'Flavanoids', 'Nonflavanoid phenols',\n",
    "                    'Proanthocyanins',\n",
    "                    'Color intensity', 'Hue',\n",
    "                    'OD280/OD315 of diluted wines',\n",
    "                    'Proline']\n",
    "\n",
    "\n",
    "# Drop 1 class\n",
    "df_wine = df_wine.loc[df_wine['Class label'] != 1]\n",
    "\n",
    "\n",
    "y = df_wine['Class label'].values\n",
    "\n",
    "X = df_wine[['Alcohol', 'OD280/OD315 of diluted wines']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.833\n"
     ]
    }
   ],
   "source": [
    "# Next, encode the labels\n",
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Check the class mappings\n",
    "# le.classes_\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)\n",
    "\n",
    "\n",
    "# Fit the Decision Tree\n",
    "\n",
    "tree = tree.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = tree.predict(X_train)\n",
    "\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(f'Decision tree train/test accuracies '\n",
    "        f'{tree_train:.3f}/{tree_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag train/test accuracies 1.000/0.917\n"
     ]
    }
   ],
   "source": [
    "bag = BaggingClassifier(base_estimator=tree, n_estimators=500,\n",
    "                        max_samples=1.0, max_features=1.0, \n",
    "                        bootstrap=True, bootstrap_features=False,\n",
    "                        n_jobs=-1, random_state=1\n",
    "                        )\n",
    "\n",
    "# Fit the Bag\n",
    "\n",
    "bag = bag.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = bag.predict(X_train)\n",
    "\n",
    "y_test_pred = bag.predict(X_test)\n",
    "\n",
    "tree_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "tree_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(f'Bag train/test accuracies '\n",
    "        f'{tree_train:.3f}/{tree_test:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Although the training accuracies of the decision tree and bagging classifier are similar on the training\n",
    "dataset (both 100 percent), we can see that the bagging classifier has a slightly better generalization\n",
    "performance, as estimated on the test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying AdaBoost using scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost train/test accuracies 1.000/0.917\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              random_state=1, \n",
    "                              max_depth=1)\n",
    "\n",
    "\n",
    "\n",
    "ada = AdaBoostClassifier(base_estimator=tree,\n",
    "                            n_estimators=500,\n",
    "                            learning_rate=0.1,\n",
    "                            random_state=1)\n",
    "\n",
    "\n",
    "ada = ada.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ada.predict(X_train)\n",
    "\n",
    "y_test_pred = ada.predict(X_test)\n",
    "\n",
    "ada_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "ada_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'AdaBoost train/test accuracies '\n",
    "        f'{ada_train:.3f}/{ada_test:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INNO\\Documents\\Python Development\\Python Machine Learning - Third Edition\\.py_ml_venv\\lib\\site-packages\\xgboost\\sklearn.py:1421: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGboost train/test accuracies 0.968/0.917\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(n_estimators=1500, learning_rate=0.01,\n",
    "                          max_depth=4, random_state=1, use_label_encoder=False\n",
    "                          )\n",
    "\n",
    "\n",
    "gbm = model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = gbm.predict(X_train)\n",
    "\n",
    "y_test_pred = gbm.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "gbm_train = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "gbm_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print(f'XGboost train/test accuracies '\n",
    "        f'{gbm_train:.3f}/{gbm_test:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, I fit the gradient boosting classifier with 1,500 trees (rounds) and a learning rate of 0.01. Typically,\n",
    "a learning rate between 0.01 and 0.1 is recommended. However, remember that the learning rate\n",
    "is used for scaling the predictions from the individual rounds. So, intuitively, the lower the learning\n",
    "rate, the more estimators are required to achieve accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".py_ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fa4aba0e7ce7e269be32cb7191cf292734c5626eb0269d818f8dc5cced85180"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
